{
    "model_name_or_path": "../ckpts/Qwen/Qwen2-7B-Instruct",
    "system_prompt": "You are a good AI assistant",
    "data_home_dir": "../hf_datasets/lvwerra/stack-exchange-paired/data/",
    "train_file_dir": "finetune",
    "eval_file_dir": "evaluation",
    "content_field": "question",
    "response_field": "response_j",
    "max_train_samples": 100000,
    "max_eval_samples": 10000,
    "response_template": "<|im_start|>user\n",
    "human_template": "<|im_start|>assistant\n",
    "report_to": "wandb",
    "run_name": "sft_qwen_on_stack_exchange_paired",
    "dataset_num_proc": 64,
    "max_grad_norm": 1,
    "warmup_steps": 5,
    "learning_rate": 2e-4,
    "weight_decay": 0.01,
    "per_device_train_batch_size": 4,
    "save_total_limit": 1,
    "gradient_accumulation_steps": 1,
    "output_dir": "sft_output/sft_samsum_llama3",
    "logging_steps": 20,
    "num_train_epochs": 3,
    "max_seq_length": 3072,
    "max_steps": -1,
    "lr_scheduler_type": "cosine",
    "gradient_checkpointing": true,
    "use_peft": true,
    "lora_r": 256,
    "lora_alpha": 32
}