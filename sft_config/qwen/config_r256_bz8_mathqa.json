{
    "model_name_or_path": "../ckpts/Qwen/Qwen2-7B-Instruct",
    "system_prompt": "You are a good AI assistant",
    "sft_task_ds": "mathqa",
    "train_file_dir": "train",
    "eval_file_dir": "test",
    "content_field": [
        "Problem",
        "options"
    ],
    "response_field": [
        "annotated_formula",
        "Rationale"
    ],
    "response_template": "<|im_start|>user\n",
    "human_template": "<|im_start|>assistant\n",
    "run_name": "sft_qwen2_mathqa",
    "dataset_num_proc": 64,
    "max_grad_norm": 1,
    "warmup_steps": 5,
    "learning_rate": 2e-4,
    "weight_decay": 0.01,
    "per_device_train_batch_size": 8,
    "save_total_limit": 1,
    "gradient_accumulation_steps": 1,
    "output_dir": "sft_output/sft_samsum_llama3",
    "logging_steps": 20,
    "num_train_epochs": 3,
    "max_seq_length": 3072,
    "max_steps": -1,
    "lr_scheduler_type": "cosine",
    "gradient_checkpointing": true,
    "use_peft": true,
    "lora_r": 256,
    "lora_alpha": 32
}