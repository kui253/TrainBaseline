{
    "model_name_or_path": "../ckpts/meta-llama/Llama-2-7b-hf",
    "system_prompt": "",
    "train_file_dir": "/home/wuhaodong/stanford_alpaca/alpaca_data.json",
    "content_field": [
        "instruction",
        "input"
    ],
    "response_field": "output",
    "template_str_dir": "./formatting_template/Llama2.yaml",
    "response_template": "[/INST]",
    "human_template": [
        13,
        29966,
        829,
        14816,
        29903,
        6778,
        13,
        13
    ],
    "run_name": "sft_llama2_on_alpaca",
    "dataset_num_proc": 1,
    "max_grad_norm": 1,
    "warmup_steps": 5,
    "learning_rate": 2e-4,
    "weight_decay": 0.01,
    "per_device_train_batch_size": 8,
    "save_total_limit": 1,
    "gradient_accumulation_steps": 4,
    "output_dir": "sft_output/sft_samsum_llama2",
    "logging_steps": 20,
    "num_train_epochs": 3,
    "max_seq_length": 3072,
    "max_steps": -1,
    "lr_scheduler_type": "cosine",
    "gradient_checkpointing": true,
    "use_peft": true,
    "lora_task_type": "custom",
    "lora_target_modules": [
        "q_proj",
        "v_proj",
        "down_proj"
    ],
    "lora_r": 256,
    "lora_alpha": 32
}