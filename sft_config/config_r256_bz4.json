{
    "model_name_or_path": "../ckpts/meta-llama/Meta-Llama-3-8B-Instruct",
    "system_prompt": "Summarize this dialogue: ",
    "train_file_dir": "../hf_datasets/samsum/train.json",
    "eval_file_dir": "../hf_datasets/samsum/val.json",
    "content_field": "dialogue",
    "response_field": "summary",
    "template_str_dir": "./formatting_template/Llama3.yaml",
    "response_template": "<|start_header_id|>assistant<|end_header_id|>\n\n",
    "human_template": "<|start_header_id|>user<|end_header_id|>\n\n",
    "report_to": "wandb",
    "run_name": "sft_llama3_on_samsum",
    "dataset_num_proc": 1,
    "max_grad_norm": 1,
    "warmup_steps": 5,
    "learning_rate": 2e-4,
    "weight_decay": 0.01,
    "per_device_train_batch_size": 8,
    "save_total_limit": 1,
    "gradient_accumulation_steps": 1,
    "output_dir": "sft_output/sft_samsum_llama3",
    "logging_steps": 20,
    "num_train_epochs": 3,
    "max_seq_length": 2048,
    "max_steps": -1,
    "lr_scheduler_type": "cosine",
    "gradient_checkpointing": true,
    "use_peft": true,
    "lora_r": 256,
    "lora_alpha": 32
}